{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3346e7a8",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35618b4f",
   "metadata": {},
   "source": [
    "In this frst part, we want to randomly retrieve tweets posted by people in the early stage of the pandemic in order to analyse how COVID-19 affected people's interests and the role of social media as communication platforms. The way we use to retrieve tweets\n",
    "aims at obtaining a sample which is the least possible biased. After retrieving data for each country (in this first analysis\n",
    "we start with Serbia and Italy, out homelands, to test the scalability and feasibility of our ideas), we want to classify them according to\n",
    "the topic they present. We are interested in analysing which have been the most discussed topics to see if we get result\n",
    "similar to those shown in CoronaWiki dataset. We think this is a good way to start analysing how people's interests shifted, how people reacted to the situation and how communication has been affected by COVID-19.\n",
    "\n",
    "Please notice that the choice of focusing on the first period of the pandemic is also due to the fact that, in Task3, we are willing to test whether a higher or lower interest in COVID in the period preceding the lockdown might have been a crucial factor to slow the infection rate after the lockdown was set. To compute this analysis, we need to collect as many data as possible regarding the early stage of the pandemic and we are not allowed to use data regarding successive stages of the pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5188b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Useful libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Twitter library\n",
    "import tweepy\n",
    "\n",
    "# Math libraries\n",
    "import numpy as np\n",
    "\n",
    "#Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Natural language processing libraries\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob # to compute sentiment analysis on each tweet\n",
    "import translators as ts\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Libray to infer the topics discussed in each tweet\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "stemmer = PorterStemmer()\n",
    "# Helpers file\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e83c5",
   "metadata": {},
   "source": [
    "#### TWEETS DATASET CREATION - ITALY AND SERBIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf884f2",
   "metadata": {},
   "source": [
    "We start by creating lists containing the names of the analysed countries and the spoken languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af5d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define lists containing the names and the spoken languages in each country\n",
    "# france, danimark, germany, italy, netherlands, normway, sweden, serbia, finland, england\n",
    "analysed_countries = ['FR','DK','DE','IT','NL','NO','SE','RS','FI','GB']\n",
    "analysed_languages = ['fr','da','de','it','nl','no','sv','sr','fi','en']\n",
    "period_per_countries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900d1e7",
   "metadata": {},
   "source": [
    "In order to compute our analysis, we need to define a period of time during the pandemic. We use pagelogs and intervention data in order to define this period of interest for each country. We retrieved data during the 3 weeks preceding the lockdown, since we are interested in analyzing human reactions and behaviour during the first stage of the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b606160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pagelogs time series\n",
    "data_path = './data/'\n",
    "with open(data_path+'aggregated_timeseries.json','r') as file:\n",
    "    pagelogs_time_series = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828768bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st case</th>\n",
       "      <th>1st death</th>\n",
       "      <th>School closure</th>\n",
       "      <th>Public events banned</th>\n",
       "      <th>Lockdown</th>\n",
       "      <th>Mobility</th>\n",
       "      <th>Normalcy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-07-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-02-22</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>2020-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-02-20</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>2020-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2020-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-05-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1st case  1st death School closure Public events banned   Lockdown  \\\n",
       "lang                                                                        \n",
       "fr   2020-01-24 2020-02-14     2020-03-14           2020-03-13 2020-03-17   \n",
       "da   2020-02-27 2020-03-12     2020-03-13           2020-03-12 2020-03-18   \n",
       "de   2020-01-27 2020-03-09     2020-03-14           2020-03-22 2020-03-22   \n",
       "it   2020-01-31 2020-02-22     2020-03-05           2020-03-09 2020-03-11   \n",
       "nl   2020-02-27 2020-03-06     2020-03-11           2020-03-24        NaT   \n",
       "no   2020-02-26 2020-02-26     2020-03-13           2020-03-12 2020-03-24   \n",
       "sr   2020-03-06 2020-03-20     2020-03-15           2020-03-21 2020-03-21   \n",
       "sv   2020-01-31 2020-03-11     2020-03-18           2020-03-12        NaT   \n",
       "ko   2020-01-20 2020-02-20     2020-02-23                  NaT        NaT   \n",
       "ca   2020-01-31 2020-02-13     2020-03-12           2020-03-08 2020-03-14   \n",
       "fi   2020-01-29 2020-03-21     2020-03-16           2020-03-16        NaT   \n",
       "ja   2020-01-16 2020-02-13     2020-02-27           2020-02-25        NaT   \n",
       "en          NaT        NaT            NaT                  NaT        NaT   \n",
       "\n",
       "       Mobility   Normalcy  \n",
       "lang                        \n",
       "fr   2020-03-16 2020-07-02  \n",
       "da   2020-03-11 2020-06-05  \n",
       "de   2020-03-16 2020-07-10  \n",
       "it   2020-03-11 2020-06-26  \n",
       "nl   2020-03-16 2020-05-29  \n",
       "no   2020-03-11 2020-06-04  \n",
       "sr   2020-03-16 2020-05-02  \n",
       "sv   2020-03-11 2020-06-05  \n",
       "ko   2020-02-25 2020-04-15  \n",
       "ca   2020-03-16        NaT  \n",
       "fi   2020-03-16 2020-05-21  \n",
       "ja   2020-03-31 2020-06-14  \n",
       "en   2020-03-16 2020-05-21  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing intervention dates for each country\n",
    "interventions = pd.read_csv(data_path + 'interventions.csv', delimiter = ',',parse_dates = ['1st case','1st death','School closure',\n",
    "                                                                                            'Public events banned','Lockdown','Mobility','Normalcy'])\n",
    "interventions.set_index('lang',inplace = True)\n",
    "interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddf4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining period of interest for each country. Dates refered to 3 weeks before the lockdown\n",
    "lockdown_dates = [interventions.loc[lang,'Lockdown'] if not pd.isnull(interventions.loc[lang,'Lockdown']) else \n",
    "                  interventions.loc[lang,'Mobility'] for lang in analysed_languages]\n",
    "\n",
    "for idx,country in enumerate(analysed_countries):\n",
    "    lang = analysed_languages[idx]\n",
    "    dates = [datetime.strptime(date.split()[0], '%Y-%m-%d')  for date in list(pagelogs_time_series[lang]['sum'].keys())]\n",
    "    dates = [date for date in dates if (lockdown_dates[idx] - date < timedelta(21)) and \n",
    "             (lockdown_dates[idx] - date >  timedelta(0))]\n",
    "    period_per_countries[country] = dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7c9dd",
   "metadata": {},
   "source": [
    "We want to retrieve tweets on a daily basis. In order to reduce the bias in our data, we decide to retrieve tweets in different moments of the day which are randomly chosen. Since most of the activity was during the afternoon, we want to give more weights to these hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "febf8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a hour time window to retrieve data\n",
    "hours = [11,12,13,14,15,16,17,18,19,20]\n",
    "# We want to give more weights to part of the day closer to dinner / late afternoon. So we assign them a higher weights\n",
    "weights = np.ones(len(hours)) / 15\n",
    "weights[-5:] = weights[-5:]*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13de1b",
   "metadata": {},
   "source": [
    "We now proceed to define two helper functions to create and import the needed dataframes. As said before, we start working on tweets posted by italian and serbian people to verify the goodness of our approach. For Milestone3, we will focus on a bigger number of countries.\n",
    "Notice that, in order to filter the tweets we retrieve, we set a very specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c614636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(name_country, language, period_of_interest, time_window, prob, skip_day=1, subsample=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which creates dataframe retrieving tweets using Twitter API\n",
    "    \n",
    "    Arguments:\n",
    "        name_country: name of country from which we are retrieving tweets\n",
    "        language: languages spoken in analyised country\n",
    "        period_of_interest: dates from which we are retrieving tweets\n",
    "        time_window: list of hours from which we are retrieving tweets \n",
    "        prob: list of weights assigned to each hour in time_window\n",
    "        skip_day: step used when iterating over period of interest\n",
    "        subsample: index of sample of data retrieved\n",
    "    \"\"\"\n",
    "    # Defining a list of topics based on Coronawiki Dataset\n",
    "    topics = []\n",
    "    # Defining support structure\n",
    "    new_data = []\n",
    "    output_path = './output/'+name_country\n",
    "    if subsample != None:\n",
    "        output_path+= subsample\n",
    "    output_path+='_tweets.pkl'\n",
    "    \n",
    "    # Importing Twitter API keys\n",
    "    with open('./Data/BearerTokens.json', 'r') as f:\n",
    "        bearer_tokens = json.load(f)\n",
    "\n",
    "    # Define more than one clien\n",
    "    bearer_token1 = bearer_tokens['balsa']\n",
    "    bearer_token2 = bearer_tokens['federico']\n",
    "    # We initialize tweepy \n",
    "    client1 = tweepy.Client(bearer_token=bearer_token1, wait_on_rate_limit=True)\n",
    "    client2 = tweepy.Client(bearer_token=bearer_token2, wait_on_rate_limit=True)\n",
    "    \n",
    "    for idx in range(0,len(period_of_interest[name_country]), skip_day):\n",
    "        # We randomly choose the time of the day to retrieve tweets. We repeat the procedure two times in order to retrieve more data\n",
    "        random_hour = np.random.choice(hours, size=2, p=weights, replace = False)\n",
    "        date = period_of_interest[name_country][idx]\n",
    "        \n",
    "        # We define start and end time to retrieve (then passed as inputs for twitter.API)\n",
    "        start_time1 = datetime(date.year,date.month,date.day,random_hour[0])\n",
    "        end_time1 = datetime(date.year,date.month,date.day,random_hour[0]+2)\n",
    "        start_time2 = datetime(date.year,date.month,date.day,random_hour[1])\n",
    "        end_time2 = datetime(date.year,date.month,date.day,random_hour[1]+2)\n",
    "        \n",
    "        # We define a proper query to get tweets from the country we're interested in\n",
    "        query = \" place_country:{} lang:{} -is:retweet -has:links -has:media -has:images \\\n",
    "                                    -has:video_link -has:mentions\".format(name_country,language)\n",
    "        \n",
    "        while True:\n",
    "            tweets1 = client1.search_all_tweets( query, max_results = 30, \n",
    "                                         start_time = start_time1, end_time = end_time1,\n",
    "                                              tweet_fields  = ['text','context_annotations','id'])\n",
    "            tweets2 = client2.search_all_tweets( query, max_results = 30, \n",
    "                                         start_time = start_time2, end_time = end_time2,\n",
    "                                              tweet_fields  = ['text','context_annotations','id'])\n",
    "            \n",
    "            time.sleep(1)\n",
    "            if tweets1.data != None or tweets2.data!= None:\n",
    "                break\n",
    "            # If we do not have data, we retrieve once again\n",
    "            random_hour = np.random.choice(hours, size=2, p=weights, replace = False)\n",
    "            date = period_of_interest[name_country][idx]\n",
    "            # We define start and end time to retrieve (then passed as inputs for twitter.API)\n",
    "            start_time1 = datetime(date.year,date.month,date.day,random_hour[0])\n",
    "            end_time1 = datetime(date.year,date.month,date.day,random_hour[0]+2)\n",
    "            start_time2 = datetime(date.year,date.month,date.day,random_hour[1])\n",
    "            end_time2 = datetime(date.year,date.month,date.day,random_hour[1]+2)\n",
    "        \n",
    "        # We perform basic preprocessing operations on the first group of retrieved tweets (translation and removal of punctuations)\n",
    "        if tweets1.data != None:\n",
    "            for tweet in tweets1.data:\n",
    "                if language != 'en':\n",
    "                    text = ts.google(tweet.text)\n",
    "                else:\n",
    "                    text = tweet.text\n",
    "                # We remove punctuation\n",
    "                text = (\"\".join([ch for ch in text if ch not in string.punctuation])).lower()\n",
    "                # We remove numbers\n",
    "                text = re.sub(\"\\d+\", \"\",text).strip()\n",
    "                # We compute sentiment analysis on the given text\n",
    "                text_sentiment = TextBlob(text).sentiment\n",
    "                text_polarity, text_subjectivity = text_sentiment.polarity, text_sentiment.subjectivity\n",
    "                # We tokenize the tweet to make the work easier\n",
    "                tokenized_stemmed_version = nltk.word_tokenize(text)\n",
    "                tokenized_stemmed_version = [stemmer.stem(word) for word in tokenized_stemmed_version]\n",
    "                # Saving new datapoint in new_data list\n",
    "                if len(text) > 0:\n",
    "                    new_data.append([date,tweet.id, language,text,tokenized_stemmed_version,\n",
    "                                     tweet.context_annotations,text_polarity,text_subjectivity])\n",
    "        \n",
    "        # We perform basic preprocessing operations on the second group of retrieved tweets\n",
    "        if tweets2.data!= None\n",
    "            for tweet in tweets2.data:\n",
    "                if language != 'en':\n",
    "                    text = ts.google(tweet.text)\n",
    "                else:\n",
    "                    text = tweet.text\n",
    "                # We remove punctuation\n",
    "                text = (\"\".join([ch for ch in text if ch not in string.punctuation])).lower()\n",
    "                # We remove numbers\n",
    "                text = re.sub(\"\\d+\", \"\",text).strip()\n",
    "                # We compute sentiment analysis on the given text\n",
    "                text_sentiment = TextBlob(text).sentiment\n",
    "                text_polarity, text_subjectivity = text_sentiment.polarity, text_sentiment.subjectivity\n",
    "                # We tokenize the tweet to make the work easier\n",
    "                tokenized_stemmed_version = nltk.word_tokenize(text)\n",
    "                tokenized_stemmed_version = [stemmer.stem(word) for word in tokenized_stemmed_version]\n",
    "                # Saving new datapoint in new_data list\n",
    "                if len(text) > 0:\n",
    "                    new_data.append([date,tweet.id, language,text,tokenized_stemmed_version,\n",
    "                                     tweet.context_annotations,text_polarity,text_subjectivity])\n",
    "            \n",
    "        # We create the dataframe\n",
    "        df = pd.DataFrame(new_data, columns = ['date','id','language','tweet','tokenized_tweet_list',\n",
    "                                               'context_from_Twitter','polarity','subjectivity'])\n",
    "        df.to_pickle(output_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd3178",
   "metadata": {},
   "source": [
    "### PIPELINE TO RETRIEVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4047de1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# refer to the list in the first cell\u001b[39;00m\n\u001b[0;32m      3\u001b[0m country \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDK\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# refer to the list in the first cell\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcreate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod_per_countries\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhours\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_day\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# A small check to see how many data we retrieved\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m get_dataframe(country)\u001b[38;5;241m.\u001b[39mdrop_duplicate(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mcreate_dataframe\u001b[1;34m(name_country, language, period_of_interest, time_window, prob, skip_day, subsample)\u001b[0m\n\u001b[0;32m     68\u001b[0m     end_time2 \u001b[38;5;241m=\u001b[39m datetime(date\u001b[38;5;241m.\u001b[39myear,date\u001b[38;5;241m.\u001b[39mmonth,date\u001b[38;5;241m.\u001b[39mday,random_hour[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# We perform basic preprocessing operations on the first group of retrieved tweets (translation and removal of punctuations)\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets1\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     73\u001b[0m         text \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mgoogle(tweet\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# choose the right language and country\n",
    "language = 'da' # refer to the list in the first cell\n",
    "country = 'DK' # refer to the list in the first cell\n",
    "create_dataframe(country,language, period_per_countries,hours, weights, skip_day=1, subsample=None)\n",
    "# A small check to see how many data we retrieved\n",
    "data = get_dataframe(country).drop_duplicate(subset = ['date','id'])\n",
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
