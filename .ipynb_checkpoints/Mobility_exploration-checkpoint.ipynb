{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase we import the data required for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './wiki_pageviews_covid/data/Global_Mobility_Report.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e00f7b81bdea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./wiki_pageviews_covid/data/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Global_Mobility_Report.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './wiki_pageviews_covid/data/Global_Mobility_Report.csv'"
     ]
    }
   ],
   "source": [
    "data_folder = './wiki_pageviews_covid/data/'\n",
    "file_name = 'Global_Mobility_Report.csv'\n",
    "df = pd.read_csv(data_folder + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on a briefly analysis on what dataframe contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Which countries are involved in the data we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country_region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is required to transform the data type of **df['date']** column to have the possibility to work with dates correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For disaggregate analysis of data and for further steps of our project we concluded that it could be useful to divide the df on continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe = ['Austria','Bosnia and Herzegovina',  'Belgium', 'Bulgaria', 'Belarus',  'Switzerland',  'Czechia', 'Germany',\n",
    "          'Denmark','Estonia',  'Spain','Finland', 'France',  'United Kingdom', 'Georgia', 'Greece',  'Croatia',\n",
    "          'Hungary',  'Ireland','Israel',  'Italy',  'Liechtenstein', 'Lithuania', 'Luxembourg', 'Latvia',  'Moldova', \n",
    "           'North Macedonia',   'Malta', 'Netherlands', 'Norway', 'Poland',  'Portugal', 'Romania', 'Serbia', \n",
    "          'Sweden',  'Slovenia', 'Slovakia',  'Turkey', 'Ukraine']\n",
    "\n",
    "Asia = [ 'Afghanistan', 'Angola', 'Bangladesh', 'Bahrain', 'Hong Kong','Indonesia','India', 'Iraq','Jordan', 'Japan',\n",
    "        'Kyrgyzstan', 'Cambodia', 'South Korea', 'Kuwait','Kazakhstan', 'Laos','Sri Lanka','Myanmar (Burma)', 'Mongolia',\n",
    "         'Malaysia', 'Nepal','Oman', 'Pakistan','Qatar','Russia','Saudi Arabia','Singapore','Thailand','Tajikistan', \n",
    "         'Taiwan','Vietnam', 'Yemen']\n",
    "\n",
    "America = ['United Arab Emirates','Antigua and Barbuda','Argentina', 'Aruba','Barbados', 'Bolivia','Brazil', 'The Bahamas', 'Belize',\n",
    "           'Canada', 'Chile','Colombia','Costa Rica','Dominican Republic', 'Ecuador','Guatemala','Honduras','Haiti','Jamaica',\n",
    "           'Mauritius', 'Mexico','Nicaragua','Peru', 'Philippines','Puerto Rico','Paraguay','El Salvador','Trinidad and Tobago'\n",
    "           'United States', 'Uruguay','Venezuela', 'Panama']\n",
    "\n",
    "Africa = ['Burkina Faso', 'Benin', 'Botswana',  \"Côte d'Ivoire\", 'Cameroon', 'Cape Verde', 'Egypt','Gabon', 'Ghana', \n",
    "          'Guinea-Bissau','Kenya','Lebanon','Libya', 'Morocco','Mali','Mozambique', 'Namibia','Niger', 'Nigeria', \n",
    "           'Papua New Guinea','Rwanda','Senegal',  'Togo', 'Tanzania','Uganda','South Africa', 'Zambia','Zimbabwe','Réunion']\n",
    "\n",
    "Oceania = ['Australia', 'Fiji', 'New Zealand',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataset for each continent\n",
    "df_europe = df.loc[df['country_region'].isin(Europe)]\n",
    "df_asia = df.loc[df['country_region'].isin(Asia)]\n",
    "df_africa = df.loc[df['country_region'].isin(Africa)]\n",
    "df_america = df.loc[df['country_region'].isin(America)]\n",
    "df_oceania = df.loc[df['country_region'].isin(Oceania)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we did an example of what we did to start understanding what dataframes were describing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe['retail_and_recreation_percent_change_from_baseline'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that we would like to start with an analysis on continents, for each country we selected the aggregate data for each country. In the dataframe infact, there are the aggregated data (*sub_region_1=NaN, sub_region_2=NaN, metro_area=NaN*) but there are also disaggregated data that for this step we decided to neglect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only the aggregate data for each country in each continent\n",
    "\n",
    "df_europe_new = df_europe.loc[(df['sub_region_1'].isna())&(df['sub_region_2'].isna())&(df['metro_area'].isna())]\n",
    "df_asia_new = df_asia.loc[(df['sub_region_1'].isna())&(df['sub_region_2'].isna())&(df['metro_area'].isna())]\n",
    "df_america_new = df_america.loc[(df['sub_region_1'].isna())&(df['sub_region_2'].isna())&(df['metro_area'].isna())]\n",
    "df_oceania_new = df_oceania.loc[(df['sub_region_1'].isna())&(df['sub_region_2'].isna())&(df['metro_area'].isna())]\n",
    "df_africa_new = df_africa.loc[(df['sub_region_1'].isna())&(df['sub_region_2'].isna())&(df['metro_area'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDLING MISSING VALUES**: Before starting to plot to have an understanding on how metrics behave on different countries, we handle missing values.  \n",
    "We decided to remove for each continent the countries with more than 30 NaN (i.e. more than 1 month of observations are missing) and we did this for each category.  \n",
    "For countries with <30 NaN for a given category, we decided to fill the NaN with the *ffill method*. For us it was a good way to use this method because we do not expect a very huge difference from consecutive days (and even if there will be, it will be captured from the first date without NaN).\n",
    "\n",
    "**PREPARING THE PLOT**: After having all the datasets of continents without missing values we computed the for each date the mean of all the metrics we have because it is what we would like to plot.\n",
    "\n",
    "**PLOTTING**: for each metrics (columns in df) we have plotted the mean of the metric for all continents in order to see if those metrics have different behavior in different continents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGREGATE ANALYSIS OF CONTINENTS\n",
    "\n",
    "for cat in df.columns[-6:]:\n",
    "    #for each continent I am removing countries with >treshold  nan (missing more than 1 month of observations) in category=cat \n",
    "    treshold = 30\n",
    "    \n",
    "    #HANDLING MISSING VALUES \n",
    "    df_europe_new['to remove '+cat]=np.zeros(df_europe_new.shape[0])\n",
    "    for country in df_europe_new['country_region'].unique():\n",
    "        nan = np.sum(df_europe_new.loc[df_europe_new['country_region']==country, cat].isna())\n",
    "        if nan > treshold:\n",
    "            df_europe_new.loc[df_europe_new['country_region']==country,'to remove '+cat] = 1\n",
    "    cat_1eu = df_europe_new.loc[df_europe_new['to remove '+cat] == 0]\n",
    "           \n",
    "    df_asia_new['to remove '+cat]=np.zeros(df_asia_new.shape[0])\n",
    "    for country in df_asia_new['country_region'].unique():\n",
    "        nan = np.sum(df_asia_new.loc[df_asia_new['country_region']==country, cat].isna())\n",
    "        if nan > treshold:\n",
    "            df_asia_new.loc[df_asia_new['country_region']==country,'to remove '+cat] = 1\n",
    "    cat_1as = df_asia_new.loc[df_asia_new['to remove '+cat] == 0]\n",
    "           \n",
    "    df_america_new['to remove '+cat]=np.zeros(df_america_new.shape[0])\n",
    "    for country in df_america_new['country_region'].unique():\n",
    "        nan = np.sum(df_america_new.loc[df_america_new['country_region']==country, cat].isna())\n",
    "        if nan > treshold:\n",
    "            df_america_new.loc[df_america_new['country_region']==country,'to remove '+cat] = 1\n",
    "    cat_1am = df_america_new.loc[df_america_new['to remove '+cat] == 0]\n",
    "           \n",
    "    df_africa_new['to remove '+cat]=np.zeros(df_africa_new.shape[0])\n",
    "    for country in df_africa_new['country_region'].unique():\n",
    "        nan = np.sum(df_africa_new.loc[df_africa_new['country_region']==country, cat].isna())\n",
    "        if nan > treshold:\n",
    "            df_africa_new.loc[df_africa_new['country_region']==country,'to remove '+cat] = 1\n",
    "    cat_1af = df_africa_new.loc[df_africa_new['to remove '+cat] == 0]\n",
    "           \n",
    "    df_oceania_new['to remove '+cat]=np.zeros(df_oceania_new.shape[0])\n",
    "    for country in df_oceania_new['country_region'].unique():\n",
    "        nan = np.sum(df_oceania_new.loc[df_oceania_new['country_region']==country, cat].isna())\n",
    "        if nan > treshold:\n",
    "            df_oceania_new.loc[df_oceania_new['country_region']==country,'to remove '+cat] = 1\n",
    "    cat_1oc = df_oceania_new.loc[df_oceania_new['to remove '+cat] == 0]\n",
    "           \n",
    "    #FILLING\n",
    "    cat_1eu = cat_1eu[['date', cat]].fillna(method='ffill')\n",
    "    cat_1af = cat_1af[['date', cat]].fillna(method='ffill')\n",
    "    cat_1oc = cat_1oc[['date', cat]].fillna(method='ffill')\n",
    "    cat_1as = cat_1as[['date', cat]].fillna(method='ffill')\n",
    "    cat_1am = cat_1am[['date', cat]].fillna(method='ffill')\n",
    "    \n",
    "    #MEANS OF DATA WITHOUT NaN (over a given day)\n",
    "    means_eu = cat_1eu.groupby('date').apply(lambda x: x.mean())\n",
    "    means_am = cat_1am.groupby('date').apply(lambda x: x.mean())\n",
    "    means_af = cat_1af.groupby('date').apply(lambda x: x.mean())\n",
    "    means_oc = cat_1oc.groupby('date').apply(lambda x: x.mean())\n",
    "    means_as = cat_1as.groupby('date').apply(lambda x: x.mean())\n",
    "    \n",
    "    #PLOTTING\n",
    "    fig = plt.figure(figsize = (11,5))\n",
    "\n",
    "    plt.plot(means_eu, label = 'europe')\n",
    "    plt.plot(means_af, label = 'africa')\n",
    "    plt.plot(means_oc, label = 'oceania')\n",
    "    plt.plot(means_as, label = 'asia')\n",
    "    plt.plot(means_am, label = 'america')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.legend(loc = 'lower right', fontsize = 10)\n",
    "    plt.title(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on an analysis of World Health Organization data. We would like to start exploring if there is a similar behavior in infections trend and the trend of some of the topics cliccked in wiki. In this way we can evaluate what people are interested in at different time (and so different severity) of pandemimcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './wiki_pageviews_covid/data/'\n",
    "file_name = 'WHO-COVID-19-global-data.csv'\n",
    "WHO_df = pd.read_csv(data_folder + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHO_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(WHO_df['Date_reported'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for the previous dataframe, we change the type of date in order to have the correct type for them to properly work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHO_df['Date_reported'] = pd.to_datetime(WHO_df['Date_reported'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by analysing what happened for Italy, country from where to of us came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy = WHO_df.loc[WHO_df['Country_code']=='IT']\n",
    "df_serbia = WHO_df.loc[WHO_df['Country_code']=='RS']\n",
    "\n",
    "df_italy_riky = df_italy.loc[(df_italy['Date_reported']>='2020-02-20') & (df_italy['Date_reported']<='2020-03-20')]\n",
    "df_serbia_riky = df_serbia.loc[(df_serbia['Date_reported']>='2020-02-20') & (df_serbia['Date_reported']<='2020-03-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serbia_riky.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do comparison with wiki data, we select on WHO dataframe only the data on the time interval we have data of wikipedia clicks (last data is on 2020-07-31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy_correct_interval = df_italy.loc[df_italy['Date_reported']<='2020-07-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy_correct_interval.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(df_italy_correct_interval['Date_reported'], df_italy_correct_interval['New_cases'], label='infections')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('New cases (daily)')\n",
    "plt.title('New cases italy (daily basis)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do comparison with wiki, we import also the dataset *aggregated_timeseries.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './wiki_pageviews_covid/data/'\n",
    "file_name = 'aggregated_timeseries.json'\n",
    "df_WIKI = pd.read_json(data_folder+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WIKI_italy = pd.DataFrame.from_dict(df_WIKI['it'].topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Which are the topics for which we have data of number of click on Wikipedia page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WIKI_italy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_click_IT = df_WIKI['it'].covid #getting the dictionary of covid page on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build from the dictionary a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_click_IT = pd.DataFrame.from_dict(covid_click_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_click_IT.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_click_IT = covid_click_IT.rename(columns = {'index':'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_click_IT['Date']=pd.to_datetime(covid_click_IT['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy_correct_interval.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_click_IT=covid_click_IT.loc[covid_click_IT['Date']>='2020-01-03'] #selecting the data only in time we have also data on previous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(df_italy_correct_interval['Date_reported'],df_italy_correct_interval['New_cases'], label='infections')\n",
    "plt.plot(time_click_IT['Date'],time_click_IT['sum'], label='wiki click on Covid')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.title('infections behavior vs wiki click on covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy_correct_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_italy_correct_interval['New_cases'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_click_IT['sum'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_regplot = df_italy_correct_interval.merge(time_click_IT,left_on='Date_reported', right_on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_regplot.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x = 'New_cases',y = 'sum', data = df_for_regplot )\n",
    "g.set(xlabel = 'new daily cases', ylabel = 'clicks on wikipedia pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better comparison of the shape of the two plot, we dcided to plot the data standardise (*min-max scaling*). In this way we can observe if the peaks occur at the same date and if the main peak occur at the same time for the two plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(df_italy_correct_interval['Date_reported'], (df_italy_correct_interval['New_cases']-np.min(df_italy_correct_interval['New_cases']))/(np.max(df_italy_correct_interval['New_cases'])-np.min(df_italy_correct_interval['New_cases'])), label='infections')\n",
    "plt.plot(time_click_IT['Date'],(time_click_IT['sum']-np.min(time_click_IT['sum']))/(np.max(time_click_IT['sum'])-np.min((time_click_IT['sum']))), label='wiki click on Covid')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('standardize values')\n",
    "plt.title('infections behavior vs wiki clicks on covid page behavior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really interesting observing that peaks of infections and peaks of wiki click on covid page are shifted in time. We will probably go further the motivation of this in milestone 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serbian_population = 6844000\n",
    "italian_population = 59070000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(df_italy_riky['Date_reported'],df_italy_riky['New_cases']/italian_population, label='infections Italy')\n",
    "plt.plot(df_serbia_riky['Date_reported'],df_serbia_riky['New_cases']/serbian_population, label='infections Serbia')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('percentage of population infected')\n",
    "plt.title('infections')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
