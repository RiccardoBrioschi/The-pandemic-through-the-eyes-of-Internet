{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf615dec",
   "metadata": {},
   "source": [
    "## PART1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af4053",
   "metadata": {},
   "source": [
    "In this frst part, we want to randomly retrieve tweets posted by people during the pandemic. The way we use to retrieve tweets\n",
    "aims at obtaining a sample which is the least possible biased. After retrieving data for each country ( in this first analysis\n",
    "we start with Serbia and Italy, out homelands, to test the scalability of the code), we want to classify them according to\n",
    "the topic they present. We are interested in analysing which have been the most discussed topics to see if we get result\n",
    "similar to those shown in CoronaWiki dataset. We think this is a good way to start analysing how people's interests shifted, how people reacted to the situation and how communication has been affected by COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3b5ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Useful libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "\n",
    "# Math libraries\n",
    "import numpy as np\n",
    "\n",
    "#Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Natural language processing libraries\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob # to compute sentiment analysis on each tweet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Libray to infer the topics discussed in each tweet\n",
    "from empath import Empath \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b18b9e",
   "metadata": {},
   "source": [
    "#### TWEETS DATASET CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6a795",
   "metadata": {},
   "source": [
    "We start by creating a dataset of retrieved tweets for each analysed country. We decide to save the dataframe in pickle format \n",
    "for optimization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bcbaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define lists containing the names and the spoken languages in each country\n",
    "# france, danimark, germany, italy, netherlands, normway, sweden, serbia, finland, england\n",
    "countries = ['FR','DK','DE','IT','NL','NO','SE','RS','FI','GB']\n",
    "languages = ['fr','da','de','it','nl','no','sv','sr','fi','en']\n",
    "period_per_countries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c7adf",
   "metadata": {},
   "source": [
    "We use pagelogs data in order to define the period of interest for each country. We start retrieving data from 01/12/2019 since it is reported as the official starting date from the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91eac45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing time series\n",
    "data_path = './data/'\n",
    "with open(data_path+'aggregated_timeseries.json','r') as file:\n",
    "    pagelogs_time_series = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fce0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining period of interest for each country. Dates are retrieved starting from 01/12/2019\n",
    "for idx,country in enumerate(countries):\n",
    "    lang = languages[idx]\n",
    "    dates = [datetime.strptime(date.split()[0], '%Y-%m-%d')  for date in list(pagelogs_time_series[lang]['sum'].keys())]\n",
    "    dates = [date for date in dates if (date.year >= 2020 or (date.year == 2019 and date.month == 12))]\n",
    "    period_per_countries[country] = dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ffc15",
   "metadata": {},
   "source": [
    "We want to retrieve tweets on a daily basis. In order to reduce the bias in our data, we decide to retrieve tweets in different moments of the day which are randomly chosen. Since most of the activity was during the afternoon, we want to give more weights to these hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb533ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a hour time window to retrieve data\n",
    "hours = [11,12,13,14,15,16,17,18,19,20]\n",
    "# We want to give more weights to part of the day closer to dinner / late afternoon. So we assign them a higher weights\n",
    "weights = np.ones(len(hours)) / 15\n",
    "weights[-5:] = weights[-5:]*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5600491",
   "metadata": {},
   "source": [
    "We now proceed to deine a helper function which created the dataframes. As said before, we start working on tweets posted by italian and serbian people to verify the goodness of our approach. For Milestone3, we will focus on a bigger number of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f824d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(name_country,language,period_of_interest,time_window = hours, prob = weights, skip_day = 1):\n",
    "    \n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    lexicon = Empath()\n",
    "    # Defining a list of topics based on Coronawiki Dataset\n",
    "    topics = []\n",
    "    # Defining support structure\n",
    "    new_data = []\n",
    "    output_path = './output/'+name_country+'_tweets.pkl'\n",
    "    \n",
    "    # We initialize tweepy \n",
    "    bearer_token_balsa = \"AAAAAAAAAAAAAAAAAAAAAPXlYgEAAAAAmHO9bfJYAPCZSDa8%2BHELxeAfwgQ%3D91B98esE93293wEGbjH4JsUMe7R3wDok1ZCGNGLLvvQXzNcyBE\"\n",
    "    client = tweepy.Client(bearer_token=bearer_token_balsa, wait_on_rate_limit=True)\n",
    "    \n",
    "    for idx in range(0,len(period_of_interest), skip_day):\n",
    "        # We randomly choose the time of the day to retrieve tweets\n",
    "        random_hour = np.random.choice(hours,p = weights)\n",
    "        date = period_of_interest[name_country][idx]\n",
    "        \n",
    "        # We define start and end time to retrieve (then passed as inputs for twitter.API)\n",
    "        start_time = datetime(date.year,date.month,date.day,random_hour)\n",
    "        end_time = datetime(date.year,date.month,date.day,random_hour+2)\n",
    "        \n",
    "        # We define a proper wuery to get tweets from the country we're interested in\n",
    "        query = \" place_country:{} lang:{} -is:retweet -has:links -has:media -has:images \\\n",
    "                                    -has:video_link -has:mentions\".format(name_country,language)\n",
    "        tweets = client.search_all_tweets( query, max_results = 10, \n",
    "                                     start_time = start_time, end_time = end_time,\n",
    "                                          tweet_fields  = ['text','context_annotations'])\n",
    "        \n",
    "        # We perform basic preprocessing operations on the text (translation and removal of punctuations)\n",
    "        for tweet in tweets.data:\n",
    "            # WE NEED TO TRANSLATE\n",
    "            # We remove punctuation\n",
    "            text = (\"\".join([ch for ch in tweet.text if ch not in string.punctuation])).lower()\n",
    "            # We remove numbers\n",
    "            text = re.sub(\"\\d+\", \"\",text).strip()\n",
    "            # We compute sentiment analysis on the given text\n",
    "            text_sentiment = TextBlob(text).sentiment\n",
    "            text_polarity, text_subjectivity = text_sentiment.polarity, text_sentiment.subjectivity\n",
    "            # We infer the discussed topic using Empath()\n",
    "            # discussed_topic = lexicon.analyze(text, categories = topics, normalize = True)\n",
    "            # We tokenize the tweet to make the work easier\n",
    "            tokenized_stemmed_version = nltk.word_tokenize(text)\n",
    "            tokenized_stemmed_version = [stemmer.stem(word) for word in tokenized_stemmed_version]\n",
    "            # Saving new datapoint in new_data list\n",
    "            if len(text) > 0:\n",
    "                new_data.append([date,language,text,tokenized_stemmed_version,\n",
    "                             tweet.context_annotations,text_polarity,text_subjectivity])\n",
    "            \n",
    "        # We create the dataframe\n",
    "        df = pd.DataFrame(new_data, columns = ['date','language','tweet','tokenized_tweet_list',\n",
    "                                               'context_from_Twitter','polarity','subjectivity'])\n",
    "        df.to_pickle(output_path)\n",
    "        \n",
    "        \n",
    "def get_dataframe(name_country):\n",
    "    get_path = './output/'+name_country+'_tweets.pkl'\n",
    "    return pd.read_pickle(get_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b845073f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet_list</th>\n",
       "      <th>context_from_Twitter</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>so excited</td>\n",
       "      <td>[so, excit]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>projects include learning about and educating ...</td>\n",
       "      <td>[project, includ, learn, about, and, educ, oth...</td>\n",
       "      <td>[{'domain': {'id': '65', 'name': 'Interests an...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>is it just me or anyone else feeling a bit lik...</td>\n",
       "      <td>[is, it, just, me, or, anyon, els, feel, a, bi...</td>\n",
       "      <td>[{'domain': {'id': '123', 'name': 'Ongoing New...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>this march and april ive ordered more stuff on...</td>\n",
       "      <td>[thi, march, and, april, ive, order, more, stu...</td>\n",
       "      <td>[{'domain': {'id': '45', 'name': 'Brand Vertic...</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>love catching luke up on my fave tiktoks of th...</td>\n",
       "      <td>[love, catch, luke, up, on, my, fave, tiktok, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date language                                              tweet  \\\n",
       "0 2020-04-29       en                                         so excited   \n",
       "1 2020-04-29       en  projects include learning about and educating ...   \n",
       "2 2020-04-29       en  is it just me or anyone else feeling a bit lik...   \n",
       "3 2020-04-29       en  this march and april ive ordered more stuff on...   \n",
       "4 2020-04-29       en  love catching luke up on my fave tiktoks of th...   \n",
       "\n",
       "                                tokenized_tweet_list  \\\n",
       "0                                        [so, excit]   \n",
       "1  [project, includ, learn, about, and, educ, oth...   \n",
       "2  [is, it, just, me, or, anyon, els, feel, a, bi...   \n",
       "3  [thi, march, and, april, ive, order, more, stu...   \n",
       "4  [love, catch, luke, up, on, my, fave, tiktok, ...   \n",
       "\n",
       "                                context_from_Twitter  polarity  subjectivity  \n",
       "0                                                 []  0.375000      0.750000  \n",
       "1  [{'domain': {'id': '65', 'name': 'Interests an... -0.133333      0.533333  \n",
       "2  [{'domain': {'id': '123', 'name': 'Ongoing New...  0.266667      0.350000  \n",
       "3  [{'domain': {'id': '45', 'name': 'Brand Vertic...  0.325000      0.575000  \n",
       "4                                                 []  0.550000      0.750000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodo = {}\n",
    "periodo['GB'] = period_per_countries['GB'][150:151]\n",
    "create_dataframe('GB','en',periodo)\n",
    "df = get_dataframe('GB')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a1f2642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet_list</th>\n",
       "      <th>context_from_Twitter</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>so excited</td>\n",
       "      <td>[so, excit]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>projects include learning about and educating ...</td>\n",
       "      <td>[project, includ, learn, about, and, educ, oth...</td>\n",
       "      <td>[{'domain': {'id': '65', 'name': 'Interests an...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>is it just me or anyone else feeling a bit lik...</td>\n",
       "      <td>[is, it, just, me, or, anyon, els, feel, a, bi...</td>\n",
       "      <td>[{'domain': {'id': '123', 'name': 'Ongoing New...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>this march and april ive ordered more stuff on...</td>\n",
       "      <td>[thi, march, and, april, ive, order, more, stu...</td>\n",
       "      <td>[{'domain': {'id': '45', 'name': 'Brand Vertic...</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>love catching luke up on my fave tiktoks of th...</td>\n",
       "      <td>[love, catch, luke, up, on, my, fave, tiktok, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>mcu loki but its matt damon in dogma instead</td>\n",
       "      <td>[mcu, loki, but, it, matt, damon, in, dogma, i...</td>\n",
       "      <td>[{'domain': {'id': '10', 'name': 'Person', 'de...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>just finished recording episode  and legit wow...</td>\n",
       "      <td>[just, finish, record, episod, and, legit, wow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>oh and skin care apparently we should all have...</td>\n",
       "      <td>[oh, and, skin, care, appar, we, should, all, ...</td>\n",
       "      <td>[{'domain': {'id': '65', 'name': 'Interests an...</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.402381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>en</td>\n",
       "      <td>treated myself to apex legends ðŸ˜ŠðŸ˜Š</td>\n",
       "      <td>[treat, myself, to, apex, legend, ðŸ˜ŠðŸ˜Š]</td>\n",
       "      <td>[{'domain': {'id': '71', 'name': 'Video Game',...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date language                                              tweet  \\\n",
       "0 2020-04-29       en                                         so excited   \n",
       "1 2020-04-29       en  projects include learning about and educating ...   \n",
       "2 2020-04-29       en  is it just me or anyone else feeling a bit lik...   \n",
       "3 2020-04-29       en  this march and april ive ordered more stuff on...   \n",
       "4 2020-04-29       en  love catching luke up on my fave tiktoks of th...   \n",
       "5 2020-04-29       en       mcu loki but its matt damon in dogma instead   \n",
       "6 2020-04-29       en  just finished recording episode  and legit wow...   \n",
       "7 2020-04-29       en  oh and skin care apparently we should all have...   \n",
       "8 2020-04-29       en                  treated myself to apex legends ðŸ˜ŠðŸ˜Š   \n",
       "\n",
       "                                tokenized_tweet_list  \\\n",
       "0                                        [so, excit]   \n",
       "1  [project, includ, learn, about, and, educ, oth...   \n",
       "2  [is, it, just, me, or, anyon, els, feel, a, bi...   \n",
       "3  [thi, march, and, april, ive, order, more, stu...   \n",
       "4  [love, catch, luke, up, on, my, fave, tiktok, ...   \n",
       "5  [mcu, loki, but, it, matt, damon, in, dogma, i...   \n",
       "6  [just, finish, record, episod, and, legit, wow...   \n",
       "7  [oh, and, skin, care, appar, we, should, all, ...   \n",
       "8              [treat, myself, to, apex, legend, ðŸ˜ŠðŸ˜Š]   \n",
       "\n",
       "                                context_from_Twitter  polarity  subjectivity  \n",
       "0                                                 []  0.375000      0.750000  \n",
       "1  [{'domain': {'id': '65', 'name': 'Interests an... -0.133333      0.533333  \n",
       "2  [{'domain': {'id': '123', 'name': 'Ongoing New...  0.266667      0.350000  \n",
       "3  [{'domain': {'id': '45', 'name': 'Brand Vertic...  0.325000      0.575000  \n",
       "4                                                 []  0.550000      0.750000  \n",
       "5  [{'domain': {'id': '10', 'name': 'Person', 'de...  0.000000      0.000000  \n",
       "6                                                 []  0.300000      0.800000  \n",
       "7  [{'domain': {'id': '65', 'name': 'Interests an...  0.155556      0.402381  \n",
       "8  [{'domain': {'id': '71', 'name': 'Video Game',...  0.000000      0.000000  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
