{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf615dec",
   "metadata": {},
   "source": [
    "#### PART1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af4053",
   "metadata": {},
   "source": [
    "In this frst part, we want to randomly retrieve tweets posted by people during the pandemic. The way we use to retrieve tweets\n",
    "aims at obtaining a sample which is the least possible biased. After retrieving data for each country ( in this first analysis\n",
    "we start with Serbia and Italy, out homelands, to test the scalability of the code), we want to classify them according to\n",
    "the topic they present. We are interested in analysing which have been the most discussed topics to see if we get result\n",
    "similar to those shown in CoronaWiki dataset. We think this is a good way to start analysing how people's interests shifted, how people reacted to the situation and how communication has been affected by COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b5ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Useful libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Math libraries\n",
    "import numpy as np\n",
    "\n",
    "#Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Natural language processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Libray to infer the topics discussed in each tweet\n",
    "from empath import Empath \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b18b9e",
   "metadata": {},
   "source": [
    "TWEETS DATASET CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6a795",
   "metadata": {},
   "source": [
    "We start by creating a dataset of retrieved tweets for each analysed country. We decide to save the dataframe in pickle format \n",
    "for optimization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcbaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define lists containing the names and the spoken languages in each country\n",
    "# france, danimark, germany, italy, netherlands, normway, sweden, serbia, finland, england\n",
    "countries = ['FR','DK','DE','IT','NL','NO','SE','RS','FI','UK']\n",
    "languages = ['fr','da','de','it','nl','no','sv','sr','fi','en']\n",
    "period_per_countries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c7adf",
   "metadata": {},
   "source": [
    "We use pagelogs data in order to define the period of interest for each country. We start retrieving data from 01/12/2022 since it is reported as the official starting date from the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91eac45e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/aggregated_timeseries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing time series\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maggregated_timeseries.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     pagelogs_time_series \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/aggregated_timeseries.json'"
     ]
    }
   ],
   "source": [
    "# Importing time series\n",
    "data_path = './data/'\n",
    "with open(data_path+'aggregated_timeseries.json','r') as file:\n",
    "    pagelogs_time_series = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce0b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373a65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime, date, time\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40772392",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token_balsa = \"AAAAAAAAAAAAAAAAAAAAAPXlYgEAAAAAmHO9bfJYAPCZSDa8%2BHELxeAfwgQ%3D91B98esE93293wEGbjH4JsUMe7R3wDok1ZCGNGLLvvQXzNcyBE\"\n",
    "client = tweepy.Client(bearer_token=bearer_token_balsa, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d54b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing time series\n",
    "data_path = './data/'\n",
    "with open(data_path+'aggregated_timeseries.json','r') as file:\n",
    "    time_series = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47c883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining period of interest for each country. Dates are retrieved starting from 01/12/2019\n",
    "for idx,country in enumerate(countries):\n",
    "    lang = languages[idx]\n",
    "    dates = [datetime.strptime(date.split()[0], '%Y-%m-%d')  for date in list(time_series[lang]['sum'].keys())]\n",
    "    dates = [date for date in dates if date.year >= 2020 or (date.year == 2019 and date.month == 12)]\n",
    "    period_per_countries[country] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf3d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a hour time window to retrieve data\n",
    "hours = [11,12,13,14,15,16,17,18,19,20]\n",
    "# We want to give more weights to part of the day closer to dinner / late afternoon -> they will count twice the other\n",
    "weights = np.ones(len(hours)) / 15\n",
    "weights[-5:] = weights[-5:]*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d2a01",
   "metadata": {},
   "source": [
    "#### Do not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51f8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_hour = np.random.choice(hours,p = weights)\n",
    "date = period_per_countries['IT'][100]\n",
    "start_time = datetime(date.year,date.month,date.day,random_hour)\n",
    "end_time = datetime(date.year,date.month,date.day,random_hour+2)\n",
    "tweets = client.search_all_tweets(\" COVID lang:en place_country: GB\", max_results = 10,\n",
    "                                     start_time = start_time, end_time = end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0635f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Corona Hysteria Syndrome is actually worse than the virus itself ! \n",
      "\n",
      "https://t.co/8qauTPebAI\n",
      "None Infamous for being a super-spreader of disinformation, could @Nigel_Farage brought COVID-19 onto the @BBCNewsnight set last night? https://t.co/bDUHsd4wdE\n",
      "None @jamesrbuk Hmm. But, and hear me out, if Jesus is God and God is omnipotent then Jesus IS COVID-19. Admittedly that requires faith in the Holy Trinity.\n",
      "None Take urgent steps to help the NHS prepare for COVID-19 - Sign the Petition! https://t.co/4JMpPf4Ykj via @UKChange\n",
      "None ‚ùåü§ù\n",
      "\n",
      "Please note there will be no pre-match handshakes before tonight‚Äôs tie with @UCCSoccer due to the Covid-19 outbreak\n",
      "\n",
      "#CRFC https://t.co/cD2SYXerVA\n",
      "None With so much hype in the news about Covid-19, I thought a post on fighting off the lurgy would be appropriate.   Here are my top 5 tips on keeping healthy throughout the cold &amp; flu season:\n",
      "\n",
      "1. Make sure you maintain‚Ä¶ https://t.co/UnSC57GnRN\n",
      "None Self-isolation advice - NHS\n",
      "\n",
      "Self-isolation advice\n",
      "- #Coronavirus (COVID-19)\n",
      "\n",
      "https://t.co/mKFYHAoXcf\n",
      "None COVID-19 is now a BIG DEAL!\n",
      "None Self-isolation advice - NHS\n",
      "\n",
      "Self-isolation advice\n",
      "- #Coronavirus (COVID-19)\n",
      "\n",
      "https://t.co/pyaEq49HTx\n",
      "None Self-isolation advice - NHS\n",
      "\n",
      "Self-isolation advice\n",
      "- #Coronavirus (COVID-19)\n",
      "\n",
      "https://t.co/m5s2osZl6J\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets.data:\n",
    "    print(tweet.created_at, tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86887c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n",
      "-0.25\n",
      "0.0\n",
      "0.0\n",
      "-0.125\n",
      "0.26666666666666666\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets.data:\n",
    "    print(TextBlob(tweet.text).sentiment.polarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
